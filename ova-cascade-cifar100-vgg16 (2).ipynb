{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch_optimizer torchmetrics\n!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6FyNf86h5S8","outputId":"199e542f-9f6a-4130-8b94-d3eda9d573d5","execution":{"iopub.status.busy":"2022-09-23T08:06:12.362356Z","iopub.execute_input":"2022-09-23T08:06:12.362830Z","iopub.status.idle":"2022-09-23T08:06:25.430473Z","shell.execute_reply.started":"2022-09-23T08:06:12.362800Z","shell.execute_reply":"2022-09-23T08:06:25.428899Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_optimizer in /opt/conda/lib/python3.7/site-packages (0.3.0)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.9.3)\nRequirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from torch_optimizer) (1.11.0)\nRequirement already satisfied: pytorch-ranger>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from torch_optimizer) (0.1.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.3.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mFri Sep 23 08:06:25 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   64C    P0    41W / 250W |  12345MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport sys\nimport torch.nn.functional as F\nfrom torch_optimizer import Ranger\nfrom torchvision import datasets\nfrom torchvision import transforms,models\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\nfrom torchmetrics import Accuracy\nfrom torch.optim.lr_scheduler import CyclicLR\n\ntorch.manual_seed(43)","metadata":{"id":"PokSv1_2Hjit","colab":{"base_uri":"https://localhost:8080/"},"outputId":"231a9e68-9995-44c1-ef31-437bc2d6eb5d","execution":{"iopub.status.busy":"2022-09-23T08:06:25.434046Z","iopub.execute_input":"2022-09-23T08:06:25.434902Z","iopub.status.idle":"2022-09-23T08:06:25.448282Z","shell.execute_reply.started":"2022-09-23T08:06:25.434842Z","shell.execute_reply":"2022-09-23T08:06:25.446832Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f5afc576af0>"},"metadata":{}}]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"KCEDW_c6Hltf","execution":{"iopub.status.busy":"2022-09-23T08:06:25.450509Z","iopub.execute_input":"2022-09-23T08:06:25.451341Z","iopub.status.idle":"2022-09-23T08:06:25.460760Z","shell.execute_reply.started":"2022-09-23T08:06:25.451299Z","shell.execute_reply":"2022-09-23T08:06:25.459428Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Data pre-processing Steps**","metadata":{"id":"mKXWAoLjTHC3"}},{"cell_type":"code","source":"\ndef target_trans(target):\n  y = -torch.ones(127)\n  y[target] = 1\n\n  global labels\n  label = labels[target]\n  for j in range(27):\n    if label in get_descendants(labels[100+j]):\n      y[100+j] = 1\n\n  return y,target","metadata":{"id":"nqCIDc_goCl3","execution":{"iopub.status.busy":"2022-09-23T08:06:25.465438Z","iopub.execute_input":"2022-09-23T08:06:25.466405Z","iopub.status.idle":"2022-09-23T08:06:25.475216Z","shell.execute_reply.started":"2022-09-23T08:06:25.466363Z","shell.execute_reply":"2022-09-23T08:06:25.473708Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(size=[32,32], padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.CenterCrop(size=[32,32]),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ndataset = datasets.CIFAR100(root='data/', train=True, download=True, transform=transform_train,target_transform=target_trans)\ntest_dataset = datasets.CIFAR100(root='data/', train=False, download=True, transform=transform_test,target_transform=target_trans)\n\n\nval_size = 5000\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nbatch_size=256\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(val_ds, batch_size, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size, num_workers=4)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3-rRz8yyDWz","outputId":"cbfa22a1-35df-4665-b512-0a2a35eb8e5d","execution":{"iopub.status.busy":"2022-09-23T08:06:25.477550Z","iopub.execute_input":"2022-09-23T08:06:25.478613Z","iopub.status.idle":"2022-09-23T08:06:27.474213Z","shell.execute_reply.started":"2022-09-23T08:06:25.478469Z","shell.execute_reply":"2022-09-23T08:06:27.472886Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"#Tree Hierarchy of the labels\n\nTree = {\"entity\" : [\"nature\", \"manmade\"] , \"nature\":[\"flora\", \"bigAnimals\", \"smallAnimals\", \"aquaticAnimals\", \"scenes\"], \"manmade\" : [\"householdElectrical\", \"householdFurniture\", \"foodContainers\", \"vehicles1\",\"vehicles2\",\"largeManmadeOutdoorThings\"],\n        \"flora\": [\"fruitsAndVegetables\",\"flowers\",\"trees\"],\"bigAnimals\": [\"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"mediumSizedMammals\"],\"smallAnimals\":[\"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\"], \"aquaticAnimals\":[\"aquaticMammals\", \"fish\"],\"scenes\":[\"cloud\",\"forest\",\"mountain\",\"plain\",\"sea\"],\n        \"householdElectrical\":[\"clock\", \"keyboard\", \"lamp\", \"telephone\", \"television\"], \"householdFurniture\":[\"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\"],\"foodContainers\":[\"bottle\", \"bowl\", \"can\", \"cup\", \"plate\"], \"vehicles1\":[\"bicycle\", \"bus\", \"motorcycle\", \"pickup_truck\", \"train\"],\"vehicles2\":[\"lawn_mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\"],\"largeManmadeOutdoorThings\":[\"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"],\n        \"flowers\":[\"orchid\", \"poppy\", \"rose\", \"sunflower\", \"tulip\"],\"fruitsAndVegetables\":[\"apple\", \"mushroom\", \"orange\", \"pear\", \"sweet_pepper\"],\"trees\":[\"maple_tree\", \"oak_tree\", \"palm_tree\", \"pine_tree\", \"willow_tree\"],\"largeCarnivores\":[\"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\"],\"largeOmniAndHerbivores\":[\"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\"],\"people\":[\"baby\", \"boy\", \"girl\", \"man\", \"woman\"],\n        \"mediumSizedMammals\":[\"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\"],\"smallMammals\":[\"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\"],\"reptiles\":[\"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\"],\"insects\":[\"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\"],\"nonInsectInvertebrates\":[\"crab\", \"lobster\", \"snail\", \"spider\", \"worm\"],\"aquaticMammals\":[\"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\"],\n        \"fish\":[\"aquarium_fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"]}\n\nlabels = [\"apple\",\"aquarium_fish\",\"baby\",\"bear\",\"beaver\",\"bed\",\"bee\",\"beetle\",\"bicycle\",\"bottle\",\"bowl\",\"boy\",\"bridge\",\"bus\",\"butterfly\",\"camel\",\"can\",\"castle\",\"caterpillar\",\"cattle\",\"chair\",\"chimpanzee\",\"clock\",\"cloud\",\"cockroach\",\"couch\",\"crab\",\"crocodile\",\"cup\",\"dinosaur\",\"dolphin\",\"elephant\",\"flatfish\",\"forest\",\"fox\",\"girl\",\"hamster\",\"house\",\"kangaroo\",\"keyboard\",\"lamp\",\"lawn_mower\",\"leopard\",\"lion\",\"lizard\",\"lobster\",\"man\",\"maple_tree\",\"motorcycle\",\"mountain\",\"mouse\",\"mushroom\",\"oak_tree\",\"orange\",\"orchid\",\"otter\",\"palm_tree\",\"pear\",\"pickup_truck\",\"pine_tree\",\"plain\",\"plate\",\"poppy\",\"porcupine\",\"possum\",\"rabbit\",\"raccoon\",\"ray\",\"road\",\"rocket\",\"rose\",\"sea\",\"seal\",\"shark\",\"shrew\",\"skunk\",\"skyscraper\",\"snail\",\"snake\",\"spider\",\"squirrel\",\"streetcar\",\"sunflower\",\"sweet_pepper\",\"table\",\"tank\",\"telephone\",\"television\",\"tiger\",\"tractor\",\"train\",\"trout\",\"tulip\",\"turtle\",\"wardrobe\",\"whale\",\"willow_tree\",\"wolf\",\"woman\",\"worm\", \"flowers\",\"fruitsAndVegetables\", \"trees\", \"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"mediumSizedMammals\", \"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\", \"aquaticMammals\", \"fish\", \"flora\", \"bigAnimals\", \"smallAnimals\",\"aquaticAnimals\", \"scenes\", \"householdElectrical\", \"householdFurniture\", \"foodContainers\", \"vehicles1\", \"vehicles2\", \"largeManmadeOutdoorThings\", \"nature\" ,\"manmade\", \"entity\"]\n\n","metadata":{"id":"uLzR7JLKn7z5","execution":{"iopub.status.busy":"2022-09-23T08:06:27.476264Z","iopub.execute_input":"2022-09-23T08:06:27.476680Z","iopub.status.idle":"2022-09-23T08:06:27.494927Z","shell.execute_reply.started":"2022-09-23T08:06:27.476641Z","shell.execute_reply":"2022-09-23T08:06:27.493452Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#Level-wise representation of labels\n\nlevel = [None]*5\nlevel[0] = [\"entity\"]\nlevel[1] = [\"nature\", \"manmade\"]\nlevel[2] = [\"flora\", \"bigAnimals\", \"smallAnimals\", \"aquaticAnimals\", \"scenes\", \"householdElectrical\", \"householdFurniture\",\"foodContainers\", \"vehicles1\", \"vehicles2\",\"largeManmadeOutdoorThings\"]\nlevel[3] = [\"flowers\", \"fruitsAndVegetables\", \"trees\", \"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"medimSizedMammals\", \"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\", \"aquaticMammals\", \"fish\", \"cloud\", \"forest\", \"mountain\", \"plain\", \"sea\", \"clock\", \"keyboard\", \"lamp\", \"telephone\", \"television\", \"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\", \"bottle\", \"bowl\", \"can\", \"cup\", \"plate\", \"bicycle\", \"bus\", \"motorcycle\", \"pickup_truck\", \"train\", \"lawn_mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\", \"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"]\nlevel[4] = [\"orchid\", \"poppy\", \"rose\", \"sunflower\", \"tulip\", \"apple\", \"mushroom\", \"orange\", \"pear\", \"sweet_pepper\", \"maple_tree\", \"oak_tree\", \"palm_tree\", \"pine_tree\", \"willow_tree\", \"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\", \"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\", \"baby\", \"boy\", \"girl\", \"man\", \"woman\", \"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\", \"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\", \"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\", \"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\", \"crab\", \"lobster\", \"snail\", \"spider\", \"worm\", \"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\", \"aquarium_fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"]\n\n\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the level of the given class label\n# output   - Returns an integer which represents the level of the class label and returns -1 if it doesn't belong to any level\ndef get_level(node):\n  if (node in level[0]):\n    return 0\n  elif (node in level[1]):\n    return 1\n  elif (node in level[2]):\n    return 2\n  elif (node in level[3]):\n    return 3\n  elif (node in level[4]):\n    return 4\n  else:\n    return -1\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find all the children of the given class label using the Tree hierarchy\n# output   - Returns an list of the labels which are children and returns None if the given label is a leaf\n\ndef get_children(node):\n  if node in Tree.keys():\n    return Tree[node]    \n  else:\n    return None\n\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the parent of the given class label using the level and children function.\n# output   - Returns the parent of the class label\ndef get_parent(node):\n  l = get_level(node)\n  if l == 0: \n    return node\n  for i in level[l-1]:\n    if(get_children(i) is not None):\n      if node in get_children(i):\n        return i\n  return None\n\n# Input    - class label of the CIFAR100 dataset and the level of the label\n# Function - finds ancestors of the the given label. \n# output   - Returns the label itself if given level is greater than the label's level else returns the ancestor at level l\ndef get_ancestor(node, l):\n  h = get_level(node)\n  if l >= h:\n    return node\n  y = node\n  for i in range(h-l):\n    y = get_parent(y)\n  return y\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the descendants of the given class label including the given class label\n# output   - Returns an list of class labels which are descendants else empty list if the class label is a leaf.\ndef get_descendants(node):\n  c = get_children(node)\n  d = []\n  if c is not None:\n    for i in c:\n      d.extend(get_descendants(i)) \n    return d\n  else :\n    return [node] \n\n\n# Input    - two class labels of the CIFAR100 dataset\n# Function - Finds the distance between the given two nodes\n# output   - Returns an integer which represents the distance between class labels\ndef tree_loss(node1,node2):\n  l1 = get_level(node1)\n  l2 = get_level(node2)\n  l = min(l1,l2)\n  while l >= 0:\n    if get_ancestor(node1,l) == get_ancestor(node2,l):\n      break\n    else :\n      l = l-1\n  return ((l1 + l2) - (2*l))\n","metadata":{"id":"w1Zs8hLg-v__","execution":{"iopub.status.busy":"2022-09-23T08:06:27.499040Z","iopub.execute_input":"2022-09-23T08:06:27.499376Z","iopub.status.idle":"2022-09-23T08:06:27.521022Z","shell.execute_reply.started":"2022-09-23T08:06:27.499349Z","shell.execute_reply":"2022-09-23T08:06:27.519622Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"**OvA Cascade**","metadata":{"id":"MKOqBEToY0P5"}},{"cell_type":"code","source":"# Input    - output : Predicted output and target : True output of the input\n# Function - For each batch of inputs, finds the tree loss using OvA algorithm.\n# output   - Returns the floating integer by calculating the batch-wise tree loss\n\ndef tLoss(output,target):\n  \n  batch_size = target.size(0)\n  num_classes = output.size(1)\n\n  # Height of the hierarchy tree\n  h = 4\n  tau_values=[0,0,0,0,0]\n  val = None\n  pred = None\n  loss = 0\n  global labels\n\n  # Iterate each example in the batch\n  for i in range(batch_size):\n    t = target[i]\n\n    h = 4\n    # searching int the bottom-up manner in the hierarchy tree\n    while h >= 0:\n\n      # Clone the output array to check at every level\n      values = output.clone().detach()\n\n      # Iterate over each class\n      for j in range(num_classes):\n\n        level = get_level(labels[j])\n\n        # Values of nodes below the current level are considered in the ancestor hence we assign the min value\n        if level > h:\n          values[i,j] = -10000\n\n        elif level < h:\n          # Similary for the node which is above current level but is not a leaf node, we assign the min value\n          if get_children(labels[j]) is not None:\n            values[i,j] = -10000   \n\n      val,pred = torch.max(values[i,:],0)\n\n      # If the output is greater than the tau value at each level we consider the pred value assigned in the previous step\n      if h == 4 and val >= tau_values[4]:\n        break\n      if h == 3 and val >= tau_values[3]:\n        break\n      if h == 2 and val >= tau_values[2]:\n        break\n      if h == 1 and val >= tau_values[1]:\n        break\n      if h == 0 and val >= tau_values[0]:\n        break \n        \n      h = h - 1\n    \n    loss = loss + tree_loss(labels[t],labels[pred])\n\n  loss = loss/batch_size\n  # Average loss for the batch is returned\n  return loss\n","metadata":{"id":"t85FVO5d-0pb","execution":{"iopub.status.busy":"2022-09-23T08:06:27.522949Z","iopub.execute_input":"2022-09-23T08:06:27.523433Z","iopub.status.idle":"2022-09-23T08:06:27.539880Z","shell.execute_reply.started":"2022-09-23T08:06:27.523405Z","shell.execute_reply":"2022-09-23T08:06:27.538342Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class Hinge_Loss(torch.nn.Module):\n    \n    def __init__(self):\n        super(Hinge_Loss,self).__init__()\n        \n    def forward(self,x,y):\n\n        temp = (1 - (x * y))\n        clamp = F.relu(temp)\n        total_loss = torch.sum(clamp)/x.size(0)\n        return total_loss","metadata":{"id":"XNYRimodkQ4I","execution":{"iopub.status.busy":"2022-09-23T08:06:27.542122Z","iopub.execute_input":"2022-09-23T08:06:27.542619Z","iopub.status.idle":"2022-09-23T08:06:27.557383Z","shell.execute_reply.started":"2022-09-23T08:06:27.542580Z","shell.execute_reply":"2022-09-23T08:06:27.556039Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"num_classes = 100\nnum_epochs = 100","metadata":{"id":"9iMkqH6_j55Y","execution":{"iopub.status.busy":"2022-09-23T08:06:27.564627Z","iopub.execute_input":"2022-09-23T08:06:27.564999Z","iopub.status.idle":"2022-09-23T08:06:27.574548Z","shell.execute_reply.started":"2022-09-23T08:06:27.564959Z","shell.execute_reply":"2022-09-23T08:06:27.572759Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class VGGForCiFar100(nn.Module):\n  def __init__(self, hid_dim=5000, dropout=0.5, n_classes=100, use_fc=True, freeze=False):\n    super().__init__()\n    self.vgg = models.vgg16(pretrained=True)\n    if not use_fc:\n      self.vgg.classifier = nn.Linear(512 * 7 * 7, n_classes)\n    else:\n      self.vgg.classifier = nn.Sequential(\n              nn.Linear(512 * 7 * 7, 5000),\n              nn.ReLU(True),\n              nn.Dropout(p=dropout),\n              #nn.Linear(4096, 4096),\n              #nn.ReLU(True),\n              #nn.Dropout(p=dropout),\n              nn.Linear(5000, n_classes),\n          )\n      \n    if freeze:\n      for param in self.vgg.features.parameters():\n        param.requires_grad = False\n      for param in self.vgg.avg_pool.parameters():\n        param.requires_grad = False\n      for param in self.vgg.flatten.parameters():\n        param.requires_grad = False\n    \n\n  def forward(self, x):\n    return self.vgg(x)","metadata":{"id":"SrnRyMx9kGb0","execution":{"iopub.status.busy":"2022-09-23T08:06:27.576552Z","iopub.execute_input":"2022-09-23T08:06:27.577717Z","iopub.status.idle":"2022-09-23T08:06:27.591494Z","shell.execute_reply.started":"2022-09-23T08:06:27.577671Z","shell.execute_reply":"2022-09-23T08:06:27.589575Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"model = VGGForCiFar100(n_classes=127).to(device)\ncriterion = Hinge_Loss().to(device)\naccuracy = Accuracy(num_classes=num_classes).to(device)\noptimizer = Ranger(model.parameters(), lr=1e-3,weight_decay=1e-4) \nscheduler = CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, step_size_up=len(train_loader)//2, cycle_momentum=False)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POJuI4hEkJpK","outputId":"45424e69-6cfb-43f3-e628-54fb7b93dcb1","execution":{"iopub.status.busy":"2022-09-23T08:06:27.593503Z","iopub.execute_input":"2022-09-23T08:06:27.594376Z","iopub.status.idle":"2022-09-23T08:06:31.635010Z","shell.execute_reply.started":"2022-09-23T08:06:27.594336Z","shell.execute_reply":"2022-09-23T08:06:31.633693Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq3zl26tkK40","outputId":"09dc5d0f-abd0-4b77-a3cb-25255590ff42","execution":{"iopub.status.busy":"2022-09-23T08:06:31.637310Z","iopub.execute_input":"2022-09-23T08:06:31.637782Z","iopub.status.idle":"2022-09-23T08:06:31.650066Z","shell.execute_reply.started":"2022-09-23T08:06:31.637738Z","shell.execute_reply":"2022-09-23T08:06:31.648438Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"VGGForCiFar100(\n  (vgg): VGG(\n    (features): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU(inplace=True)\n      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (25): ReLU(inplace=True)\n      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (27): ReLU(inplace=True)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n    (classifier): Sequential(\n      (0): Linear(in_features=25088, out_features=5000, bias=True)\n      (1): ReLU(inplace=True)\n      (2): Dropout(p=0.5, inplace=False)\n      (3): Linear(in_features=5000, out_features=127, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_accuracy(output, target):\n  batch_size = target.size(0)\n\n  pred = output[:,0:100]      # considering the first 100 of the 127 nodes because they are the true classes(No internal node)\n  pred = pred.max(dim=1)[1]\n\n  correct = pred==target\n  acc = correct.float().sum(0)\n\n  return acc/batch_size","metadata":{"execution":{"iopub.status.busy":"2022-09-23T08:06:31.651312Z","iopub.execute_input":"2022-09-23T08:06:31.651732Z","iopub.status.idle":"2022-09-23T08:06:31.660807Z","shell.execute_reply.started":"2022-09-23T08:06:31.651675Z","shell.execute_reply":"2022-09-23T08:06:31.659151Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"**Training and Validation**","metadata":{"id":"KiMoL2NmWzsT"}},{"cell_type":"code","source":"#Training\n\ntotal_train_step = len(train_loader)\n#print(total_train_step)\ntotal_val_step=len(valid_loader)\nBEST_VAL_METRIC = 0\nBEST_MODEL = None\n\n\nfor epoch in range(1, num_epochs+1):\n\n    train_loss=0\n    train_acc=0.0\n    model.train()\n\n    for i, (images, target) in enumerate(train_loader, 1):\n\n        y_trans = target[0]\n        y_true = target[1]\n\n        # Move tensors to the configured device\n        images = images.to(device)\n        y_true = y_true.to(device)\n        y_trans = y_trans.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, y_trans)\n\n        train_loss += loss\n        train_acc += get_accuracy(outputs, y_true)\n        #train_acc += accuracy(outputs[:,0:100], y_true)\n        \n        \n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # scheduler.step(train_loss/total_train_step)\n        \n\n    print(f'Epoch [{epoch}/{num_epochs}] - Loss: {(train_loss/total_train_step):.4f}, Accuracy: {(train_acc/total_train_step):.4f}')\n\n    model.eval() \n    # Validation\n    with torch.no_grad():\n        val_acc = 0\n        val_loss=0\n        Tree_Loss_Value = 0\n        for i, (images, target) in enumerate(valid_loader, 1):\n\n            y_trans = target[0]\n            y_true = target[1]\n\n            # Move tensors to the configured device\n            images = images.to(device)\n            y_true = y_true.to(device)\n            y_trans = y_trans.to(device)\n\n            outputs = model(images)\n            val_loss += criterion(outputs, y_trans)\n            Tree_Loss_Value += tLoss(outputs,y_true)\n            val_acc += get_accuracy(outputs, y_true)\n            #val_acc += accuracy(outputs[:,0:100], y_true)\n\n    if val_acc/total_val_step > BEST_VAL_METRIC:\n        BEST_VAL_METRIC = val_acc/total_val_step\n        BEST_MODEL = model.state_dict() \n\n    print(f'Accuracy of the network on validation images: {(val_acc/total_val_step):.4f}, loss: {(val_loss/total_val_step):.4f}, Tree loss: {(Tree_Loss_Value/total_val_step):.4f}') ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zE0L6pzBKlZW","outputId":"c7a62bd9-b36d-4abc-9adc-7201abc05294","execution":{"iopub.status.busy":"2022-09-23T08:06:31.663263Z","iopub.execute_input":"2022-09-23T08:06:31.663747Z","iopub.status.idle":"2022-09-23T09:33:15.036563Z","shell.execute_reply.started":"2022-09-23T08:06:31.663693Z","shell.execute_reply":"2022-09-23T09:33:15.034738Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Epoch [1/100] - Loss: 17.9507, Accuracy: 0.0665\nAccuracy of the network on validation images: 0.2272, loss: 5.8182, Tree loss: 2.7839\nEpoch [2/100] - Loss: 5.5816, Accuracy: 0.1822\nAccuracy of the network on validation images: 0.3281, loss: 4.9392, Tree loss: 2.4604\nEpoch [3/100] - Loss: 4.8552, Accuracy: 0.2918\nAccuracy of the network on validation images: 0.4001, loss: 4.4262, Tree loss: 2.2474\nEpoch [4/100] - Loss: 4.3643, Accuracy: 0.3735\nAccuracy of the network on validation images: 0.4446, loss: 4.1038, Tree loss: 2.0890\nEpoch [5/100] - Loss: 4.0092, Accuracy: 0.4315\nAccuracy of the network on validation images: 0.4796, loss: 3.8846, Tree loss: 1.9870\nEpoch [6/100] - Loss: 3.7283, Accuracy: 0.4717\nAccuracy of the network on validation images: 0.5000, loss: 3.6859, Tree loss: 1.8753\nEpoch [7/100] - Loss: 3.4842, Accuracy: 0.5116\nAccuracy of the network on validation images: 0.5283, loss: 3.5871, Tree loss: 1.8166\nEpoch [8/100] - Loss: 3.2350, Accuracy: 0.5415\nAccuracy of the network on validation images: 0.5420, loss: 3.4602, Tree loss: 1.7373\nEpoch [9/100] - Loss: 3.0827, Accuracy: 0.5647\nAccuracy of the network on validation images: 0.5629, loss: 3.3657, Tree loss: 1.6637\nEpoch [10/100] - Loss: 2.9333, Accuracy: 0.5867\nAccuracy of the network on validation images: 0.5722, loss: 3.2796, Tree loss: 1.6221\nEpoch [11/100] - Loss: 2.8069, Accuracy: 0.6020\nAccuracy of the network on validation images: 0.5747, loss: 3.2589, Tree loss: 1.6208\nEpoch [12/100] - Loss: 2.6989, Accuracy: 0.6143\nAccuracy of the network on validation images: 0.5773, loss: 3.2157, Tree loss: 1.5993\nEpoch [13/100] - Loss: 2.5917, Accuracy: 0.6296\nAccuracy of the network on validation images: 0.5979, loss: 3.1355, Tree loss: 1.5697\nEpoch [14/100] - Loss: 2.4726, Accuracy: 0.6469\nAccuracy of the network on validation images: 0.5963, loss: 3.1392, Tree loss: 1.5307\nEpoch [15/100] - Loss: 2.4066, Accuracy: 0.6519\nAccuracy of the network on validation images: 0.5983, loss: 3.1184, Tree loss: 1.5337\nEpoch [16/100] - Loss: 2.3185, Accuracy: 0.6656\nAccuracy of the network on validation images: 0.6009, loss: 3.0928, Tree loss: 1.5062\nEpoch [17/100] - Loss: 2.2502, Accuracy: 0.6769\nAccuracy of the network on validation images: 0.6088, loss: 3.0423, Tree loss: 1.5024\nEpoch [18/100] - Loss: 2.1853, Accuracy: 0.6841\nAccuracy of the network on validation images: 0.6074, loss: 3.0408, Tree loss: 1.4808\nEpoch [19/100] - Loss: 2.1223, Accuracy: 0.6909\nAccuracy of the network on validation images: 0.6119, loss: 2.9938, Tree loss: 1.4681\nEpoch [20/100] - Loss: 2.0637, Accuracy: 0.7009\nAccuracy of the network on validation images: 0.6169, loss: 3.0209, Tree loss: 1.4722\nEpoch [21/100] - Loss: 2.0591, Accuracy: 0.7059\nAccuracy of the network on validation images: 0.6213, loss: 2.9640, Tree loss: 1.4376\nEpoch [22/100] - Loss: 1.9717, Accuracy: 0.7164\nAccuracy of the network on validation images: 0.6235, loss: 2.9704, Tree loss: 1.4254\nEpoch [23/100] - Loss: 1.9088, Accuracy: 0.7229\nAccuracy of the network on validation images: 0.6283, loss: 2.9375, Tree loss: 1.4089\nEpoch [24/100] - Loss: 1.8551, Accuracy: 0.7302\nAccuracy of the network on validation images: 0.6261, loss: 2.9970, Tree loss: 1.4299\nEpoch [25/100] - Loss: 1.8491, Accuracy: 0.7323\nAccuracy of the network on validation images: 0.6259, loss: 2.9728, Tree loss: 1.4304\nEpoch [26/100] - Loss: 1.7768, Accuracy: 0.7411\nAccuracy of the network on validation images: 0.6233, loss: 2.9367, Tree loss: 1.4242\nEpoch [27/100] - Loss: 1.7333, Accuracy: 0.7476\nAccuracy of the network on validation images: 0.6318, loss: 3.0151, Tree loss: 1.4438\nEpoch [28/100] - Loss: 1.6840, Accuracy: 0.7539\nAccuracy of the network on validation images: 0.6393, loss: 2.9068, Tree loss: 1.3823\nEpoch [29/100] - Loss: 1.6574, Accuracy: 0.7606\nAccuracy of the network on validation images: 0.6324, loss: 2.9782, Tree loss: 1.4019\nEpoch [30/100] - Loss: 1.6443, Accuracy: 0.7626\nAccuracy of the network on validation images: 0.6328, loss: 3.0132, Tree loss: 1.4220\nEpoch [31/100] - Loss: 1.5749, Accuracy: 0.7692\nAccuracy of the network on validation images: 0.6377, loss: 2.9765, Tree loss: 1.4057\nEpoch [32/100] - Loss: 1.5913, Accuracy: 0.7704\nAccuracy of the network on validation images: 0.6337, loss: 2.9694, Tree loss: 1.4138\nEpoch [33/100] - Loss: 1.5754, Accuracy: 0.7743\nAccuracy of the network on validation images: 0.6394, loss: 2.9904, Tree loss: 1.4079\nEpoch [34/100] - Loss: 1.5477, Accuracy: 0.7760\nAccuracy of the network on validation images: 0.6437, loss: 2.9411, Tree loss: 1.3960\nEpoch [35/100] - Loss: 1.5004, Accuracy: 0.7804\nAccuracy of the network on validation images: 0.6407, loss: 2.9624, Tree loss: 1.3672\nEpoch [36/100] - Loss: 1.4657, Accuracy: 0.7863\nAccuracy of the network on validation images: 0.6367, loss: 3.0535, Tree loss: 1.4287\nEpoch [37/100] - Loss: 1.4514, Accuracy: 0.7889\nAccuracy of the network on validation images: 0.6429, loss: 3.0138, Tree loss: 1.4072\nEpoch [38/100] - Loss: 1.4478, Accuracy: 0.7920\nAccuracy of the network on validation images: 0.6380, loss: 3.0317, Tree loss: 1.3883\nEpoch [39/100] - Loss: 1.4219, Accuracy: 0.7938\nAccuracy of the network on validation images: 0.6326, loss: 3.1121, Tree loss: 1.4400\nEpoch [40/100] - Loss: 1.4033, Accuracy: 0.7972\nAccuracy of the network on validation images: 0.6390, loss: 3.0322, Tree loss: 1.4209\nEpoch [41/100] - Loss: 1.3723, Accuracy: 0.8013\nAccuracy of the network on validation images: 0.6333, loss: 3.0118, Tree loss: 1.3998\nEpoch [42/100] - Loss: 1.3537, Accuracy: 0.8059\nAccuracy of the network on validation images: 0.6343, loss: 3.1090, Tree loss: 1.4187\nEpoch [43/100] - Loss: 1.3053, Accuracy: 0.8084\nAccuracy of the network on validation images: 0.6380, loss: 3.1304, Tree loss: 1.4346\nEpoch [44/100] - Loss: 1.2990, Accuracy: 0.8119\nAccuracy of the network on validation images: 0.6443, loss: 3.0634, Tree loss: 1.4113\nEpoch [45/100] - Loss: 1.3136, Accuracy: 0.8104\nAccuracy of the network on validation images: 0.6417, loss: 3.1655, Tree loss: 1.4112\nEpoch [46/100] - Loss: 1.2874, Accuracy: 0.8145\nAccuracy of the network on validation images: 0.6334, loss: 3.1119, Tree loss: 1.4167\nEpoch [47/100] - Loss: 1.2882, Accuracy: 0.8157\nAccuracy of the network on validation images: 0.6431, loss: 3.0279, Tree loss: 1.3795\nEpoch [48/100] - Loss: 1.2139, Accuracy: 0.8243\nAccuracy of the network on validation images: 0.6352, loss: 3.1561, Tree loss: 1.4254\nEpoch [49/100] - Loss: 1.2135, Accuracy: 0.8240\nAccuracy of the network on validation images: 0.6478, loss: 3.0200, Tree loss: 1.3635\nEpoch [50/100] - Loss: 1.1739, Accuracy: 0.8290\nAccuracy of the network on validation images: 0.6405, loss: 3.0892, Tree loss: 1.4077\nEpoch [51/100] - Loss: 1.1915, Accuracy: 0.8250\nAccuracy of the network on validation images: 0.6360, loss: 3.1663, Tree loss: 1.4328\nEpoch [52/100] - Loss: 1.1664, Accuracy: 0.8319\nAccuracy of the network on validation images: 0.6513, loss: 3.0805, Tree loss: 1.4007\nEpoch [53/100] - Loss: 1.1411, Accuracy: 0.8335\nAccuracy of the network on validation images: 0.6376, loss: 3.1784, Tree loss: 1.4406\nEpoch [54/100] - Loss: 1.1203, Accuracy: 0.8389\nAccuracy of the network on validation images: 0.6531, loss: 3.1185, Tree loss: 1.3848\nEpoch [55/100] - Loss: 1.1246, Accuracy: 0.8380\nAccuracy of the network on validation images: 0.6479, loss: 3.1243, Tree loss: 1.3886\nEpoch [56/100] - Loss: 1.1175, Accuracy: 0.8382\nAccuracy of the network on validation images: 0.6438, loss: 3.1484, Tree loss: 1.3894\nEpoch [57/100] - Loss: 1.0695, Accuracy: 0.8440\nAccuracy of the network on validation images: 0.6390, loss: 3.1243, Tree loss: 1.4232\nEpoch [58/100] - Loss: 1.0463, Accuracy: 0.8478\nAccuracy of the network on validation images: 0.6456, loss: 3.1825, Tree loss: 1.4144\nEpoch [59/100] - Loss: 1.0609, Accuracy: 0.8488\nAccuracy of the network on validation images: 0.6413, loss: 3.1108, Tree loss: 1.3908\nEpoch [60/100] - Loss: 1.0861, Accuracy: 0.8449\nAccuracy of the network on validation images: 0.6299, loss: 3.2246, Tree loss: 1.4351\nEpoch [61/100] - Loss: 1.0272, Accuracy: 0.8511\nAccuracy of the network on validation images: 0.6350, loss: 3.3220, Tree loss: 1.4450\nEpoch [62/100] - Loss: 1.0208, Accuracy: 0.8512\nAccuracy of the network on validation images: 0.6359, loss: 3.1995, Tree loss: 1.4347\nEpoch [63/100] - Loss: 1.0528, Accuracy: 0.8489\nAccuracy of the network on validation images: 0.6446, loss: 3.2373, Tree loss: 1.4367\nEpoch [64/100] - Loss: 1.0186, Accuracy: 0.8547\nAccuracy of the network on validation images: 0.6471, loss: 3.1031, Tree loss: 1.3783\nEpoch [65/100] - Loss: 1.0001, Accuracy: 0.8552\nAccuracy of the network on validation images: 0.6387, loss: 3.2523, Tree loss: 1.4212\nEpoch [66/100] - Loss: 1.0152, Accuracy: 0.8545\nAccuracy of the network on validation images: 0.6385, loss: 3.2124, Tree loss: 1.4291\nEpoch [67/100] - Loss: 0.9752, Accuracy: 0.8601\nAccuracy of the network on validation images: 0.6472, loss: 3.2444, Tree loss: 1.4213\nEpoch [68/100] - Loss: 0.9686, Accuracy: 0.8603\nAccuracy of the network on validation images: 0.6422, loss: 3.2208, Tree loss: 1.3939\nEpoch [69/100] - Loss: 0.9397, Accuracy: 0.8661\nAccuracy of the network on validation images: 0.6467, loss: 3.2077, Tree loss: 1.4123\nEpoch [70/100] - Loss: 0.9189, Accuracy: 0.8676\nAccuracy of the network on validation images: 0.6491, loss: 3.1693, Tree loss: 1.3715\nEpoch [71/100] - Loss: 0.9876, Accuracy: 0.8599\nAccuracy of the network on validation images: 0.6421, loss: 3.2611, Tree loss: 1.4408\nEpoch [72/100] - Loss: 0.9612, Accuracy: 0.8630\nAccuracy of the network on validation images: 0.6412, loss: 3.2838, Tree loss: 1.4157\nEpoch [73/100] - Loss: 0.9816, Accuracy: 0.8625\nAccuracy of the network on validation images: 0.6410, loss: 3.2698, Tree loss: 1.4369\nEpoch [74/100] - Loss: 0.9215, Accuracy: 0.8679\nAccuracy of the network on validation images: 0.6420, loss: 3.2918, Tree loss: 1.4243\nEpoch [75/100] - Loss: 0.8987, Accuracy: 0.8691\nAccuracy of the network on validation images: 0.6438, loss: 3.1915, Tree loss: 1.3782\nEpoch [76/100] - Loss: 0.8776, Accuracy: 0.8723\nAccuracy of the network on validation images: 0.6428, loss: 3.2028, Tree loss: 1.3949\nEpoch [77/100] - Loss: 0.8904, Accuracy: 0.8729\nAccuracy of the network on validation images: 0.6460, loss: 3.1930, Tree loss: 1.4024\nEpoch [78/100] - Loss: 0.8829, Accuracy: 0.8717\nAccuracy of the network on validation images: 0.6395, loss: 3.2787, Tree loss: 1.4188\nEpoch [79/100] - Loss: 0.8674, Accuracy: 0.8754\nAccuracy of the network on validation images: 0.6486, loss: 3.1912, Tree loss: 1.3680\nEpoch [80/100] - Loss: 0.8336, Accuracy: 0.8796\nAccuracy of the network on validation images: 0.6438, loss: 3.2409, Tree loss: 1.4278\nEpoch [81/100] - Loss: 0.8722, Accuracy: 0.8776\nAccuracy of the network on validation images: 0.6478, loss: 3.1770, Tree loss: 1.3705\nEpoch [82/100] - Loss: 0.8400, Accuracy: 0.8803\nAccuracy of the network on validation images: 0.6459, loss: 3.3549, Tree loss: 1.4340\nEpoch [83/100] - Loss: 0.8694, Accuracy: 0.8788\nAccuracy of the network on validation images: 0.6493, loss: 3.2340, Tree loss: 1.3851\nEpoch [84/100] - Loss: 0.8454, Accuracy: 0.8818\nAccuracy of the network on validation images: 0.6440, loss: 3.4011, Tree loss: 1.4289\nEpoch [85/100] - Loss: 0.8123, Accuracy: 0.8823\nAccuracy of the network on validation images: 0.6423, loss: 3.2851, Tree loss: 1.4234\nEpoch [86/100] - Loss: 0.8280, Accuracy: 0.8830\nAccuracy of the network on validation images: 0.6493, loss: 3.2681, Tree loss: 1.3994\nEpoch [87/100] - Loss: 0.7865, Accuracy: 0.8881\nAccuracy of the network on validation images: 0.6463, loss: 3.2784, Tree loss: 1.4013\nEpoch [88/100] - Loss: 0.7969, Accuracy: 0.8872\nAccuracy of the network on validation images: 0.6507, loss: 3.2715, Tree loss: 1.3973\nEpoch [89/100] - Loss: 0.8273, Accuracy: 0.8842\nAccuracy of the network on validation images: 0.6494, loss: 3.3976, Tree loss: 1.4352\nEpoch [90/100] - Loss: 0.7977, Accuracy: 0.8892\nAccuracy of the network on validation images: 0.6526, loss: 3.3227, Tree loss: 1.3916\nEpoch [91/100] - Loss: 0.7643, Accuracy: 0.8921\nAccuracy of the network on validation images: 0.6421, loss: 3.4101, Tree loss: 1.4323\nEpoch [92/100] - Loss: 0.7517, Accuracy: 0.8939\nAccuracy of the network on validation images: 0.6476, loss: 3.3178, Tree loss: 1.3842\nEpoch [93/100] - Loss: 0.7882, Accuracy: 0.8896\nAccuracy of the network on validation images: 0.6395, loss: 3.4214, Tree loss: 1.4393\nEpoch [94/100] - Loss: 0.8163, Accuracy: 0.8860\nAccuracy of the network on validation images: 0.6443, loss: 3.3059, Tree loss: 1.3884\nEpoch [95/100] - Loss: 0.7865, Accuracy: 0.8899\nAccuracy of the network on validation images: 0.6470, loss: 3.3374, Tree loss: 1.3810\nEpoch [96/100] - Loss: 0.7423, Accuracy: 0.8947\nAccuracy of the network on validation images: 0.6527, loss: 3.3683, Tree loss: 1.3914\nEpoch [97/100] - Loss: 0.8227, Accuracy: 0.8886\nAccuracy of the network on validation images: 0.6436, loss: 3.3115, Tree loss: 1.4073\nEpoch [98/100] - Loss: 0.7725, Accuracy: 0.8922\nAccuracy of the network on validation images: 0.6511, loss: 3.2805, Tree loss: 1.3938\nEpoch [99/100] - Loss: 0.7552, Accuracy: 0.8953\nAccuracy of the network on validation images: 0.6493, loss: 3.3332, Tree loss: 1.3814\nEpoch [100/100] - Loss: 0.7264, Accuracy: 0.8974\nAccuracy of the network on validation images: 0.6515, loss: 3.3532, Tree loss: 1.3914\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing**","metadata":{"id":"8Q01Kk42XffV"}},{"cell_type":"code","source":"\n#Testing\nmodel.load_state_dict(BEST_MODEL)\n\ntotal_test_step=len(test_loader)\n\nwith torch.no_grad():\n    test_acc=0\n    test_loss=0\n    Tree_Loss_Value=0\n\n    for i, (images, target) in enumerate(test_loader, 1):\n        \n        y_trans = target[0]\n        y_true = target[1]\n        \n        images = images.to(device)\n        y_true = y_true.to(device)\n        y_trans = y_trans.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        \n        # Loss\n        test_loss += criterion(outputs,y_trans)\n        Tree_Loss_Value += tLoss(outputs,y_true)\n        test_acc += get_accuracy(outputs, y_true)\n        #test_acc += accuracy(outputs[:,0:100], y_true)\n\n    print(f'Accuracy of the network on test images: {(test_acc/total_test_step):.4f}, loss: {(test_loss/total_test_step):.4f}, Tree loss: {(Tree_Loss_Value/total_test_step):.4f}')","metadata":{"id":"1eV5Njd2K3fn","execution":{"iopub.status.busy":"2022-09-23T09:33:15.040238Z","iopub.execute_input":"2022-09-23T09:33:15.040769Z","iopub.status.idle":"2022-09-23T09:33:38.957261Z","shell.execute_reply.started":"2022-09-23T09:33:15.040699Z","shell.execute_reply":"2022-09-23T09:33:38.955643Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Accuracy of the network on test images: 0.6692, loss: 3.1885, Tree loss: 1.3100\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"W50Zq-zsufa7"},"execution_count":null,"outputs":[]}]}