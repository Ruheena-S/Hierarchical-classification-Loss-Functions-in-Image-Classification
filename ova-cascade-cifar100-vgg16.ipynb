{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch_optimizer torchmetrics\n!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6FyNf86h5S8","outputId":"199e542f-9f6a-4130-8b94-d3eda9d573d5","execution":{"iopub.status.busy":"2022-09-22T03:53:37.637948Z","iopub.execute_input":"2022-09-22T03:53:37.638602Z","iopub.status.idle":"2022-09-22T03:53:48.763458Z","shell.execute_reply.started":"2022-09-22T03:53:37.638550Z","shell.execute_reply":"2022-09-22T03:53:48.762122Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_optimizer in /opt/conda/lib/python3.7/site-packages (0.3.0)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.9.3)\nRequirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from torch_optimizer) (1.11.0)\nRequirement already satisfied: pytorch-ranger>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from torch_optimizer) (0.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.3.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mThu Sep 22 03:53:48 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   53C    P0    36W / 250W |  15221MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport sys\nimport torch.nn.functional as F\nfrom torch_optimizer import Ranger\nfrom torchvision import datasets\nfrom torchvision import transforms,models\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\nfrom torchmetrics import Accuracy\nfrom torch.optim.lr_scheduler import CyclicLR\n\ntorch.manual_seed(43)","metadata":{"id":"PokSv1_2Hjit","colab":{"base_uri":"https://localhost:8080/"},"outputId":"231a9e68-9995-44c1-ef31-437bc2d6eb5d","execution":{"iopub.status.busy":"2022-09-22T03:53:48.766338Z","iopub.execute_input":"2022-09-22T03:53:48.766733Z","iopub.status.idle":"2022-09-22T03:53:48.776143Z","shell.execute_reply.started":"2022-09-22T03:53:48.766690Z","shell.execute_reply":"2022-09-22T03:53:48.775132Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f2acc214ab0>"},"metadata":{}}]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"KCEDW_c6Hltf","execution":{"iopub.status.busy":"2022-09-22T03:53:48.777394Z","iopub.execute_input":"2022-09-22T03:53:48.779213Z","iopub.status.idle":"2022-09-22T03:53:48.788728Z","shell.execute_reply.started":"2022-09-22T03:53:48.779177Z","shell.execute_reply":"2022-09-22T03:53:48.787722Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Data pre-processing Steps**","metadata":{"id":"mKXWAoLjTHC3"}},{"cell_type":"code","source":"\ndef target_trans(target):\n  y = -torch.ones(127)\n  y[target] = 1\n\n  global labels\n  label = labels[target]\n  for j in range(27):\n    if label in get_descendants(labels[100+j]):\n      y[100+j] = 1\n\n  return y,target","metadata":{"id":"nqCIDc_goCl3","execution":{"iopub.status.busy":"2022-09-22T03:53:48.792618Z","iopub.execute_input":"2022-09-22T03:53:48.793210Z","iopub.status.idle":"2022-09-22T03:53:48.799614Z","shell.execute_reply.started":"2022-09-22T03:53:48.793183Z","shell.execute_reply":"2022-09-22T03:53:48.798512Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(size=[32,32], padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.CenterCrop(size=[32,32]),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ndataset = datasets.CIFAR100(root='data/', train=True, download=True, transform=transform_train,target_transform=target_trans)\ntest_dataset = datasets.CIFAR100(root='data/', train=False, download=True, transform=transform_test,target_transform=target_trans)\n\n\nval_size = 10000\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nbatch_size=256\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(val_ds, batch_size, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size, num_workers=4)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3-rRz8yyDWz","outputId":"cbfa22a1-35df-4665-b512-0a2a35eb8e5d","execution":{"iopub.status.busy":"2022-09-22T03:53:48.800986Z","iopub.execute_input":"2022-09-22T03:53:48.802036Z","iopub.status.idle":"2022-09-22T03:53:50.601044Z","shell.execute_reply.started":"2022-09-22T03:53:48.802001Z","shell.execute_reply":"2022-09-22T03:53:50.600050Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"#Tree Hierarchy of the labels\n\nTree = {\"entity\" : [\"nature\", \"manmade\"] , \"nature\":[\"flora\", \"bigAnimals\", \"smallAnimals\", \"aquaticAnimals\", \"scenes\"], \"manmade\" : [\"householdElectrical\", \"householdFurniture\", \"foodContainers\", \"vehicles1\",\"vehicles2\",\"largeManmadeOutdoorThings\"],\n        \"flora\": [\"fruitsAndVegetables\",\"flowers\",\"trees\"],\"bigAnimals\": [\"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"mediumSizedMammals\"],\"smallAnimals\":[\"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\"], \"aquaticAnimals\":[\"aquaticMammals\", \"fish\"],\"scenes\":[\"cloud\",\"forest\",\"mountain\",\"plain\",\"sea\"],\n        \"householdElectrical\":[\"clock\", \"keyboard\", \"lamp\", \"telephone\", \"television\"], \"householdFurniture\":[\"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\"],\"foodContainers\":[\"bottle\", \"bowl\", \"can\", \"cup\", \"plate\"], \"vehicles1\":[\"bicycle\", \"bus\", \"motorcycle\", \"pickup_truck\", \"train\"],\"vehicles2\":[\"lawn_mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\"],\"largeManmadeOutdoorThings\":[\"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"],\n        \"flowers\":[\"orchid\", \"poppy\", \"rose\", \"sunflower\", \"tulip\"],\"fruitsAndVegetables\":[\"apple\", \"mushroom\", \"orange\", \"pear\", \"sweet_pepper\"],\"trees\":[\"maple_tree\", \"oak_tree\", \"palm_tree\", \"pine_tree\", \"willow_tree\"],\"largeCarnivores\":[\"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\"],\"largeOmniAndHerbivores\":[\"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\"],\"people\":[\"baby\", \"boy\", \"girl\", \"man\", \"woman\"],\n        \"mediumSizedMammals\":[\"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\"],\"smallMammals\":[\"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\"],\"reptiles\":[\"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\"],\"insects\":[\"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\"],\"nonInsectInvertebrates\":[\"crab\", \"lobster\", \"snail\", \"spider\", \"worm\"],\"aquaticMammals\":[\"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\"],\n        \"fish\":[\"aquarium_fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"]}\n\nlabels = [\"apple\",\"aquarium_fish\",\"baby\",\"bear\",\"beaver\",\"bed\",\"bee\",\"beetle\",\"bicycle\",\"bottle\",\"bowl\",\"boy\",\"bridge\",\"bus\",\"butterfly\",\"camel\",\"can\",\"castle\",\"caterpillar\",\"cattle\",\"chair\",\"chimpanzee\",\"clock\",\"cloud\",\"cockroach\",\"couch\",\"crab\",\"crocodile\",\"cup\",\"dinosaur\",\"dolphin\",\"elephant\",\"flatfish\",\"forest\",\"fox\",\"girl\",\"hamster\",\"house\",\"kangaroo\",\"keyboard\",\"lamp\",\"lawn_mower\",\"leopard\",\"lion\",\"lizard\",\"lobster\",\"man\",\"maple_tree\",\"motorcycle\",\"mountain\",\"mouse\",\"mushroom\",\"oak_tree\",\"orange\",\"orchid\",\"otter\",\"palm_tree\",\"pear\",\"pickup_truck\",\"pine_tree\",\"plain\",\"plate\",\"poppy\",\"porcupine\",\"possum\",\"rabbit\",\"raccoon\",\"ray\",\"road\",\"rocket\",\"rose\",\"sea\",\"seal\",\"shark\",\"shrew\",\"skunk\",\"skyscraper\",\"snail\",\"snake\",\"spider\",\"squirrel\",\"streetcar\",\"sunflower\",\"sweet_pepper\",\"table\",\"tank\",\"telephone\",\"television\",\"tiger\",\"tractor\",\"train\",\"trout\",\"tulip\",\"turtle\",\"wardrobe\",\"whale\",\"willow_tree\",\"wolf\",\"woman\",\"worm\", \"flowers\",\"fruitsAndVegetables\", \"trees\", \"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"mediumSizedMammals\", \"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\", \"aquaticMammals\", \"fish\", \"flora\", \"bigAnimals\", \"smallAnimals\",\"aquaticAnimals\", \"scenes\", \"householdElectrical\", \"householdFurniture\", \"foodContainers\", \"vehicles1\", \"vehicles2\", \"largeManmadeOutdoorThings\", \"nature\" ,\"manmade\", \"entity\"]\n\n","metadata":{"id":"uLzR7JLKn7z5","execution":{"iopub.status.busy":"2022-09-22T03:53:50.602564Z","iopub.execute_input":"2022-09-22T03:53:50.603161Z","iopub.status.idle":"2022-09-22T03:53:50.619449Z","shell.execute_reply.started":"2022-09-22T03:53:50.603118Z","shell.execute_reply":"2022-09-22T03:53:50.618454Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#Level-wise representation of labels\n\nlevel = [None]*5\nlevel[0] = [\"entity\"]\nlevel[1] = [\"nature\", \"manmade\"]\nlevel[2] = [\"flora\", \"bigAnimals\", \"smallAnimals\", \"aquaticAnimals\", \"scenes\", \"householdElectrical\", \"householdFurniture\",\"foodContainers\", \"vehicles1\", \"vehicles2\",\"largeManmadeOutdoorThings\"]\nlevel[3] = [\"flowers\", \"fruitsAndVegetables\", \"trees\", \"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"medimSizedMammals\", \"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\", \"aquaticMammals\", \"fish\", \"cloud\", \"forest\", \"mountain\", \"plain\", \"sea\", \"clock\", \"keyboard\", \"lamp\", \"telephone\", \"television\", \"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\", \"bottle\", \"bowl\", \"can\", \"cup\", \"plate\", \"bicycle\", \"bus\", \"motorcycle\", \"pickup_truck\", \"train\", \"lawn_mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\", \"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"]\nlevel[4] = [\"orchid\", \"poppy\", \"rose\", \"sunflower\", \"tulip\", \"apple\", \"mushroom\", \"orange\", \"pear\", \"sweet_pepper\", \"maple_tree\", \"oak_tree\", \"palm_tree\", \"pine_tree\", \"willow_tree\", \"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\", \"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\", \"baby\", \"boy\", \"girl\", \"man\", \"woman\", \"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\", \"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\", \"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\", \"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\", \"crab\", \"lobster\", \"snail\", \"spider\", \"worm\", \"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\", \"aquarium_fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"]\n\n\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the level of the given class label\n# output   - Returns an integer which represents the level of the class label and returns -1 if it doesn't belong to any level\ndef get_level(node):\n  if (node in level[0]):\n    return 0\n  elif (node in level[1]):\n    return 1\n  elif (node in level[2]):\n    return 2\n  elif (node in level[3]):\n    return 3\n  elif (node in level[4]):\n    return 4\n  else:\n    return -1\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find all the children of the given class label using the Tree hierarchy\n# output   - Returns an list of the labels which are children and returns None if the given label is a leaf\n\ndef get_children(node):\n  if node in Tree.keys():\n    return Tree[node]    \n  else:\n    return None\n'''\ndef get_children(x):\n  if x == \"entity\":\n    return [\"nature\", \"manmade\"]\n  elif x == \"nature\":\n    return [\"flora\", \"bigAnimals\", \"smallAnimals\", \"aquaticAnimals\", \"scenes\"]\n  elif x == \"manmade\":\n    return [\"householdElectrical\", \"householdFurniture\", \"foodContainers\", \"vehicles1\",\"vehicles2\",\"largeManmadeOutdoorThings\"]\n  elif x == \"flora\":\n    return[\"fruitsAndVegetables\",\"flowers\",\"trees\"]\n  elif x == \"bigAnimals\":\n    return [\"largeCarnivores\", \"largeOmniAndHerbivores\", \"people\", \"mediumSizedMammals\"]\n  elif x == \"smallAnimals\":\n    return [\"smallMammals\", \"reptiles\", \"insects\", \"nonInsectInvertebrates\"]\n  elif x== \"aquaticAnimals\":\n    return [\"aquaticMammals\", \"fish\"]\n  elif x == \"scenes\":\n    return [\"cloud\",\"forest\",\"mountain\",\"plain\",\"sea\"]\n  elif x ==  \"householdElectrical\":\n    return [\"clock\", \"keyboard\", \"lamp\", \"telephone\", \"television\"]\n  elif x ==  \"householdFurniture\":\n    return [\"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\"]\n  elif x ==  \"foodContainers\":\n    return [\"bottle\", \"bowl\", \"can\", \"cup\", \"plate\"]\n  elif x ==  \"vehicles1\":\n    return [\"bicycle\", \"bus\", \"motorcycle\", \"pickup_truck\", \"train\"]\n  elif x ==  \"vehicles2\":\n    return [\"lawn_mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\"]\n  elif x ==  \"largeManmadeOutdoorThings\":\n    return [\"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"]\n  elif x ==  \"flowers\":\n    return [\"orchid\", \"poppy\", \"rose\", \"sunflower\", \"tulip\"]\n  elif x ==  \"fruitsAndVegetables\":\n    return [\"apple\", \"mushroom\", \"orange\", \"pear\", \"sweet_pepper\"]\n  elif x ==  \"trees\":\n    return [\"maple_tree\", \"oak_tree\", \"palm_tree\", \"pine_tree\", \"willow_tree\"]\n  elif x == \"largeCarnivores\":\n    return [\"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\"]\n  elif x ==  \"largeOmniAndHerbivores\":\n    return [\"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\"]\n  elif x ==  \"people\":\n    return [\"baby\", \"boy\", \"girl\", \"man\", \"woman\"]\n  elif x ==  \"mediumSizedMammals\":\n    return [\"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\"]\n  elif x ==  \"smallMammals\":\n    return [\"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\"]\n  elif x ==  \"reptiles\":\n    return [\"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\"]\n  elif x ==  \"insects\":\n    return [\"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\"]\n  elif x ==  \"nonInsectInvertebrates\":\n    return [\"crab\", \"lobster\", \"snail\", \"spider\", \"worm\"]\n  elif x ==  \"aquaticMammals\":\n    return [\"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\"]\n  elif x ==  \"fish\":\n    return [\"aquarium_fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"] \n  else:\n    return None\n'''\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the parent of the given class label using the level and children function.\n# output   - Returns the parent of the class label\ndef get_parent(node):\n  l = get_level(node)\n  if l == 0: \n    return node\n  for i in level[l-1]:\n    if(get_children(i) is not None):\n      if node in get_children(i):\n        return i\n  return None\n\n# Input    - class label of the CIFAR100 dataset and the level of the label\n# Function - finds ancestors of the the given label. \n# output   - Returns the label itself if given level is greater than the label's level else returns the ancestor at level l\ndef get_ancestor(node, l):\n  h = get_level(node)\n  if l >= h:\n    return node\n  y = node\n  for i in range(h-l):\n    y = get_parent(y)\n  return y\n\n# Input    - class label of the CIFAR100 dataset\n# Function - To find the descendants of the given class label including the given class label\n# output   - Returns an list of class labels which are descendants else empty list if the class label is a leaf.\ndef get_descendants(node):\n  c = get_children(node)\n  d = []\n  if c is not None:\n    for i in c:\n      d.extend(get_descendants(i)) \n    return d\n  else :\n    return [node] \n\n\n# Input    - two class labels of the CIFAR100 dataset\n# Function - Finds the distance between the given two nodes\n# output   - Returns an integer which represents the distance between class labels\ndef tree_loss(node1,node2):\n  l1 = get_level(node1)\n  l2 = get_level(node2)\n  l = min(l1,l2)\n  while l >= 0:\n    if get_ancestor(node1,l) == get_ancestor(node2,l):\n      break\n    else :\n      l = l-1\n  return ((l1 + l2) - (2*l))\n","metadata":{"id":"w1Zs8hLg-v__","execution":{"iopub.status.busy":"2022-09-22T03:53:50.621106Z","iopub.execute_input":"2022-09-22T03:53:50.621488Z","iopub.status.idle":"2022-09-22T03:53:50.642572Z","shell.execute_reply.started":"2022-09-22T03:53:50.621453Z","shell.execute_reply":"2022-09-22T03:53:50.641598Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"**OvA Cascade**","metadata":{"id":"MKOqBEToY0P5"}},{"cell_type":"code","source":"# Input    - output : Predicted output and target : True output of the input\n# Function - For each batch of inputs, finds the tree loss using OvA algorithm.\n# output   - Returns the floating integer by calculating the batch-wise tree loss\n\ndef tLoss(output,target):\n  \n  batch_size = target.size(0)\n  num_classes = output.size(1)\n\n  # Height of the hierarchy tree\n  h = 4\n  tau_values=[0,0,0,0,0]\n  val = None\n  pred = None\n  loss = 0\n  global labels\n\n  # Iterate each example in the batch\n  for i in range(batch_size):\n    t = target[i]\n\n    h = 4\n    # searching int the bottom-up manner in the hierarchy tree\n    while h >= 0:\n\n      # Clone the output array to check at every level\n      values = output.clone().detach()\n\n      # Iterate over each class\n      for j in range(num_classes):\n\n        level = get_level(labels[j])\n\n        # Values of nodes below the current level are considered in the ancestor hence we assign the min value\n        if level > h:\n          values[i,j] = -10000\n\n        elif level < h:\n          # Similary for the node which is above current level but is not a leaf node, we assign the min value\n          if get_children(labels[j]) is not None:\n            values[i,j] = -10000   \n\n      val,pred = torch.max(values[i,:],0)\n\n      # If the output is greater than the tau value at each level we consider the pred value assigned in the previous step\n      if h == 4 and val >= tau_values[4]:\n        break\n      if h == 3 and val >= tau_values[3]:\n        break\n      if h == 2 and val >= tau_values[2]:\n        break\n      if h == 1 and val >= tau_values[1]:\n        break\n      if h == 0 and val >= tau_values[0]:\n        break \n        \n      h = h - 1\n    \n    loss = loss + tree_loss(labels[t],labels[pred])\n\n  loss = loss/batch_size\n  # Average loss for the batch is returned\n  return loss\n","metadata":{"id":"t85FVO5d-0pb","execution":{"iopub.status.busy":"2022-09-22T03:53:50.643707Z","iopub.execute_input":"2022-09-22T03:53:50.644894Z","iopub.status.idle":"2022-09-22T03:53:50.657222Z","shell.execute_reply.started":"2022-09-22T03:53:50.644858Z","shell.execute_reply":"2022-09-22T03:53:50.656566Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class Hinge_Loss(torch.nn.Module):\n    \n    def __init__(self):\n        super(Hinge_Loss,self).__init__()\n        \n    def forward(self,x,y):\n\n        temp = (1 - (x * y))\n        clamp = F.relu(temp)\n        total_loss = torch.sum(clamp)/x.size(0)\n        return total_loss","metadata":{"id":"XNYRimodkQ4I","execution":{"iopub.status.busy":"2022-09-22T03:53:50.658472Z","iopub.execute_input":"2022-09-22T03:53:50.659423Z","iopub.status.idle":"2022-09-22T03:53:50.670425Z","shell.execute_reply.started":"2022-09-22T03:53:50.659387Z","shell.execute_reply":"2022-09-22T03:53:50.669488Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"num_classes = 100\nnum_epochs = 100","metadata":{"id":"9iMkqH6_j55Y","execution":{"iopub.status.busy":"2022-09-22T03:53:50.673774Z","iopub.execute_input":"2022-09-22T03:53:50.674469Z","iopub.status.idle":"2022-09-22T03:53:50.683712Z","shell.execute_reply.started":"2022-09-22T03:53:50.674441Z","shell.execute_reply":"2022-09-22T03:53:50.682780Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class VGGForCiFar100(nn.Module):\n  def __init__(self, hid_dim=5000, dropout=0.5, n_classes=100, use_fc=True, freeze=False):\n    super().__init__()\n    self.vgg = models.vgg16(pretrained=True)\n    if not use_fc:\n      self.vgg.classifier = nn.Linear(512 * 7 * 7, n_classes)\n    else:\n      self.vgg.classifier = nn.Sequential(\n              nn.Linear(512 * 7 * 7, 5000),\n              nn.ReLU(True),\n              nn.Dropout(p=dropout),\n              #nn.Linear(4096, 4096),\n              #nn.ReLU(True),\n              #nn.Dropout(p=dropout),\n              nn.Linear(5000, n_classes),\n          )\n      \n    if freeze:\n      for param in self.vgg.features.parameters():\n        param.requires_grad = False\n      for param in self.vgg.avg_pool.parameters():\n        param.requires_grad = False\n      for param in self.vgg.flatten.parameters():\n        param.requires_grad = False\n    \n\n  def forward(self, x):\n    return self.vgg(x)","metadata":{"id":"SrnRyMx9kGb0","execution":{"iopub.status.busy":"2022-09-22T03:53:50.685056Z","iopub.execute_input":"2022-09-22T03:53:50.686017Z","iopub.status.idle":"2022-09-22T03:53:50.695135Z","shell.execute_reply.started":"2022-09-22T03:53:50.685979Z","shell.execute_reply":"2022-09-22T03:53:50.694449Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"model = VGGForCiFar100(n_classes=127).to(device)\ncriterion = Hinge_Loss().to(device)\naccuracy = Accuracy(num_classes=num_classes).to(device)\noptimizer = Ranger(model.parameters(), lr=1e-3,weight_decay=1e-4) \nscheduler = CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, step_size_up=len(train_loader)//2, cycle_momentum=False)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POJuI4hEkJpK","outputId":"45424e69-6cfb-43f3-e628-54fb7b93dcb1","execution":{"iopub.status.busy":"2022-09-22T03:53:50.696333Z","iopub.execute_input":"2022-09-22T03:53:50.697280Z","iopub.status.idle":"2022-09-22T03:53:54.018574Z","shell.execute_reply.started":"2022-09-22T03:53:50.697245Z","shell.execute_reply":"2022-09-22T03:53:54.017537Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq3zl26tkK40","outputId":"09dc5d0f-abd0-4b77-a3cb-25255590ff42","execution":{"iopub.status.busy":"2022-09-22T03:53:54.020022Z","iopub.execute_input":"2022-09-22T03:53:54.020771Z","iopub.status.idle":"2022-09-22T03:53:54.029122Z","shell.execute_reply.started":"2022-09-22T03:53:54.020732Z","shell.execute_reply":"2022-09-22T03:53:54.027877Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"VGGForCiFar100(\n  (vgg): VGG(\n    (features): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU(inplace=True)\n      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (25): ReLU(inplace=True)\n      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (27): ReLU(inplace=True)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n    (classifier): Sequential(\n      (0): Linear(in_features=25088, out_features=5000, bias=True)\n      (1): ReLU(inplace=True)\n      (2): Dropout(p=0.5, inplace=False)\n      (3): Linear(in_features=5000, out_features=127, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"**Training and Validation**","metadata":{"id":"KiMoL2NmWzsT"}},{"cell_type":"code","source":"#Training\n\ntotal_train_step = len(train_loader)\n#print(total_train_step)\ntotal_val_step=len(valid_loader)\nBEST_VAL_METRIC = 0\nBEST_MODEL = None\n\n\nfor epoch in range(1, num_epochs+1):\n\n    train_loss=0\n    train_acc=0.0\n    model.train()\n\n    for i, (images, target) in enumerate(train_loader, 1):\n\n        y_trans = target[0]\n        y_true = target[1]\n\n        # Move tensors to the configured device\n        images = images.to(device)\n        y_true = y_true.to(device)\n        y_trans = y_trans.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, y_trans)\n\n        train_loss += loss\n        # train_acc += get_accuracy(outputs, y_true)\n        train_acc += accuracy(outputs[:,0:100], y_true)\n        \n        \n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # scheduler.step(train_loss/total_train_step)\n        \n\n    print(f'Epoch [{epoch}/{num_epochs}] - Loss: {(train_loss/total_train_step):.4f}, Accuracy: {(train_acc/total_train_step):.4f}')\n\n    model.eval() \n    # Validation\n    with torch.no_grad():\n        val_acc = 0\n        val_loss=0\n        Tree_Loss_Value = 0\n        for i, (images, target) in enumerate(valid_loader, 1):\n\n            y_trans = target[0]\n            y_true = target[1]\n\n            # Move tensors to the configured device\n            images = images.to(device)\n            y_true = y_true.to(device)\n            y_trans = y_trans.to(device)\n\n            outputs = model(images)\n            val_loss += criterion(outputs, y_trans)\n            Tree_Loss_Value += tLoss(outputs,y_true)\n            # val_acc += get_accuracy(outputs, y_true)\n            val_acc += accuracy(outputs[:,0:100], y_true)\n\n    if val_acc/total_val_step > BEST_VAL_METRIC:\n        BEST_VAL_METRIC = val_acc/total_val_step\n        BEST_MODEL = model.state_dict() \n\n    print(f'Accuracy of the network on validation images: {(val_acc/total_val_step):.4f}, loss: {(val_loss/total_val_step):.4f}, Tree loss: {(Tree_Loss_Value/total_val_step):.4f}') ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zE0L6pzBKlZW","outputId":"c7a62bd9-b36d-4abc-9adc-7201abc05294","execution":{"iopub.status.busy":"2022-09-22T03:53:54.031402Z","iopub.execute_input":"2022-09-22T03:53:54.031824Z","iopub.status.idle":"2022-09-22T05:21:08.079611Z","shell.execute_reply.started":"2022-09-22T03:53:54.031779Z","shell.execute_reply":"2022-09-22T05:21:08.077832Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch [1/100] - Loss: 18.7739, Accuracy: 0.0537\nAccuracy of the network on validation images: 0.1954, loss: 6.0127, Tree loss: 2.8570\nEpoch [2/100] - Loss: 5.7534, Accuracy: 0.1574\nAccuracy of the network on validation images: 0.3029, loss: 5.1035, Tree loss: 2.5400\nEpoch [3/100] - Loss: 5.0395, Accuracy: 0.2578\nAccuracy of the network on validation images: 0.3577, loss: 4.6101, Tree loss: 2.3156\nEpoch [4/100] - Loss: 4.5426, Accuracy: 0.3311\nAccuracy of the network on validation images: 0.4194, loss: 4.2450, Tree loss: 2.1693\nEpoch [5/100] - Loss: 4.1936, Accuracy: 0.3927\nAccuracy of the network on validation images: 0.4543, loss: 4.0063, Tree loss: 2.0626\nEpoch [6/100] - Loss: 3.8880, Accuracy: 0.4407\nAccuracy of the network on validation images: 0.4782, loss: 3.8152, Tree loss: 1.9534\nEpoch [7/100] - Loss: 3.6511, Accuracy: 0.4787\nAccuracy of the network on validation images: 0.5099, loss: 3.6487, Tree loss: 1.8512\nEpoch [8/100] - Loss: 3.4360, Accuracy: 0.5114\nAccuracy of the network on validation images: 0.5260, loss: 3.4926, Tree loss: 1.7684\nEpoch [9/100] - Loss: 3.2591, Accuracy: 0.5376\nAccuracy of the network on validation images: 0.5339, loss: 3.4725, Tree loss: 1.7490\nEpoch [10/100] - Loss: 3.0813, Accuracy: 0.5588\nAccuracy of the network on validation images: 0.5594, loss: 3.3363, Tree loss: 1.6749\nEpoch [11/100] - Loss: 2.9236, Accuracy: 0.5825\nAccuracy of the network on validation images: 0.5718, loss: 3.2851, Tree loss: 1.6500\nEpoch [12/100] - Loss: 2.8058, Accuracy: 0.6002\nAccuracy of the network on validation images: 0.5804, loss: 3.1845, Tree loss: 1.5784\nEpoch [13/100] - Loss: 2.6730, Accuracy: 0.6191\nAccuracy of the network on validation images: 0.5888, loss: 3.1493, Tree loss: 1.5567\nEpoch [14/100] - Loss: 2.5589, Accuracy: 0.6345\nAccuracy of the network on validation images: 0.5917, loss: 3.1363, Tree loss: 1.5562\nEpoch [15/100] - Loss: 2.4680, Accuracy: 0.6440\nAccuracy of the network on validation images: 0.5946, loss: 3.1246, Tree loss: 1.5460\nEpoch [16/100] - Loss: 2.3815, Accuracy: 0.6572\nAccuracy of the network on validation images: 0.6014, loss: 3.1246, Tree loss: 1.5231\nEpoch [17/100] - Loss: 2.3344, Accuracy: 0.6677\nAccuracy of the network on validation images: 0.5955, loss: 3.1421, Tree loss: 1.5339\nEpoch [18/100] - Loss: 2.2436, Accuracy: 0.6771\nAccuracy of the network on validation images: 0.6061, loss: 3.0666, Tree loss: 1.4918\nEpoch [19/100] - Loss: 2.1898, Accuracy: 0.6858\nAccuracy of the network on validation images: 0.6093, loss: 3.0368, Tree loss: 1.4853\nEpoch [20/100] - Loss: 2.1005, Accuracy: 0.6985\nAccuracy of the network on validation images: 0.6106, loss: 3.0896, Tree loss: 1.4675\nEpoch [21/100] - Loss: 2.0506, Accuracy: 0.7057\nAccuracy of the network on validation images: 0.6124, loss: 3.0413, Tree loss: 1.4631\nEpoch [22/100] - Loss: 1.9913, Accuracy: 0.7114\nAccuracy of the network on validation images: 0.6170, loss: 3.0595, Tree loss: 1.4570\nEpoch [23/100] - Loss: 1.9045, Accuracy: 0.7210\nAccuracy of the network on validation images: 0.6080, loss: 3.0958, Tree loss: 1.4750\nEpoch [24/100] - Loss: 1.8623, Accuracy: 0.7283\nAccuracy of the network on validation images: 0.6251, loss: 2.9998, Tree loss: 1.4388\nEpoch [25/100] - Loss: 1.8614, Accuracy: 0.7293\nAccuracy of the network on validation images: 0.6271, loss: 2.9453, Tree loss: 1.4074\nEpoch [26/100] - Loss: 1.7828, Accuracy: 0.7395\nAccuracy of the network on validation images: 0.6219, loss: 3.0473, Tree loss: 1.4408\nEpoch [27/100] - Loss: 1.7838, Accuracy: 0.7436\nAccuracy of the network on validation images: 0.6218, loss: 3.0553, Tree loss: 1.4408\nEpoch [28/100] - Loss: 1.7404, Accuracy: 0.7479\nAccuracy of the network on validation images: 0.6180, loss: 3.1497, Tree loss: 1.4815\nEpoch [29/100] - Loss: 1.6480, Accuracy: 0.7580\nAccuracy of the network on validation images: 0.6192, loss: 3.1217, Tree loss: 1.4461\nEpoch [30/100] - Loss: 1.6116, Accuracy: 0.7661\nAccuracy of the network on validation images: 0.6261, loss: 3.0218, Tree loss: 1.4238\nEpoch [31/100] - Loss: 1.6011, Accuracy: 0.7666\nAccuracy of the network on validation images: 0.6396, loss: 2.9712, Tree loss: 1.3817\nEpoch [32/100] - Loss: 1.5837, Accuracy: 0.7723\nAccuracy of the network on validation images: 0.6397, loss: 2.9746, Tree loss: 1.3970\nEpoch [33/100] - Loss: 1.5420, Accuracy: 0.7767\nAccuracy of the network on validation images: 0.6318, loss: 3.0671, Tree loss: 1.4187\nEpoch [34/100] - Loss: 1.5346, Accuracy: 0.7753\nAccuracy of the network on validation images: 0.6316, loss: 3.0988, Tree loss: 1.4197\nEpoch [35/100] - Loss: 1.4813, Accuracy: 0.7857\nAccuracy of the network on validation images: 0.6335, loss: 3.0853, Tree loss: 1.4166\nEpoch [36/100] - Loss: 1.4856, Accuracy: 0.7875\nAccuracy of the network on validation images: 0.6337, loss: 3.0500, Tree loss: 1.4126\nEpoch [37/100] - Loss: 1.3926, Accuracy: 0.7983\nAccuracy of the network on validation images: 0.6388, loss: 3.0430, Tree loss: 1.3898\nEpoch [38/100] - Loss: 1.3812, Accuracy: 0.7991\nAccuracy of the network on validation images: 0.6292, loss: 3.1225, Tree loss: 1.4266\nEpoch [39/100] - Loss: 1.3149, Accuracy: 0.8074\nAccuracy of the network on validation images: 0.6337, loss: 3.1372, Tree loss: 1.4379\nEpoch [40/100] - Loss: 1.3760, Accuracy: 0.8042\nAccuracy of the network on validation images: 0.6295, loss: 3.2354, Tree loss: 1.4627\nEpoch [41/100] - Loss: 1.3474, Accuracy: 0.8061\nAccuracy of the network on validation images: 0.6199, loss: 3.2706, Tree loss: 1.4635\nEpoch [42/100] - Loss: 1.3511, Accuracy: 0.8072\nAccuracy of the network on validation images: 0.6395, loss: 3.0636, Tree loss: 1.3938\nEpoch [43/100] - Loss: 1.2784, Accuracy: 0.8143\nAccuracy of the network on validation images: 0.6359, loss: 3.1182, Tree loss: 1.4148\nEpoch [44/100] - Loss: 1.2416, Accuracy: 0.8194\nAccuracy of the network on validation images: 0.6292, loss: 3.2160, Tree loss: 1.4381\nEpoch [45/100] - Loss: 1.2400, Accuracy: 0.8203\nAccuracy of the network on validation images: 0.6270, loss: 3.2727, Tree loss: 1.4520\nEpoch [46/100] - Loss: 1.1863, Accuracy: 0.8291\nAccuracy of the network on validation images: 0.6201, loss: 3.3693, Tree loss: 1.5038\nEpoch [47/100] - Loss: 1.2271, Accuracy: 0.8241\nAccuracy of the network on validation images: 0.6215, loss: 3.2425, Tree loss: 1.4659\nEpoch [48/100] - Loss: 1.2551, Accuracy: 0.8207\nAccuracy of the network on validation images: 0.6417, loss: 3.1228, Tree loss: 1.3793\nEpoch [49/100] - Loss: 1.1763, Accuracy: 0.8295\nAccuracy of the network on validation images: 0.6349, loss: 3.2265, Tree loss: 1.4345\nEpoch [50/100] - Loss: 1.2249, Accuracy: 0.8253\nAccuracy of the network on validation images: 0.6327, loss: 3.2938, Tree loss: 1.4530\nEpoch [51/100] - Loss: 1.1428, Accuracy: 0.8370\nAccuracy of the network on validation images: 0.6342, loss: 3.2389, Tree loss: 1.4229\nEpoch [52/100] - Loss: 1.1226, Accuracy: 0.8394\nAccuracy of the network on validation images: 0.6207, loss: 3.4928, Tree loss: 1.5141\nEpoch [53/100] - Loss: 1.1718, Accuracy: 0.8341\nAccuracy of the network on validation images: 0.6143, loss: 3.5068, Tree loss: 1.5384\nEpoch [54/100] - Loss: 1.1487, Accuracy: 0.8355\nAccuracy of the network on validation images: 0.6459, loss: 3.1494, Tree loss: 1.4032\nEpoch [55/100] - Loss: 1.1474, Accuracy: 0.8366\nAccuracy of the network on validation images: 0.6338, loss: 3.2802, Tree loss: 1.4377\nEpoch [56/100] - Loss: 1.1488, Accuracy: 0.8391\nAccuracy of the network on validation images: 0.6393, loss: 3.1970, Tree loss: 1.4080\nEpoch [57/100] - Loss: 1.0164, Accuracy: 0.8515\nAccuracy of the network on validation images: 0.6287, loss: 3.2969, Tree loss: 1.4569\nEpoch [58/100] - Loss: 0.9952, Accuracy: 0.8562\nAccuracy of the network on validation images: 0.6154, loss: 3.4407, Tree loss: 1.5072\nEpoch [59/100] - Loss: 1.0087, Accuracy: 0.8558\nAccuracy of the network on validation images: 0.6188, loss: 3.5423, Tree loss: 1.5000\nEpoch [60/100] - Loss: 1.0052, Accuracy: 0.8591\nAccuracy of the network on validation images: 0.6417, loss: 3.3038, Tree loss: 1.4095\nEpoch [61/100] - Loss: 0.9809, Accuracy: 0.8602\nAccuracy of the network on validation images: 0.6343, loss: 3.3445, Tree loss: 1.4525\nEpoch [62/100] - Loss: 0.9701, Accuracy: 0.8612\nAccuracy of the network on validation images: 0.6336, loss: 3.4241, Tree loss: 1.4622\nEpoch [63/100] - Loss: 0.9946, Accuracy: 0.8600\nAccuracy of the network on validation images: 0.6264, loss: 3.4033, Tree loss: 1.4737\nEpoch [64/100] - Loss: 0.9564, Accuracy: 0.8628\nAccuracy of the network on validation images: 0.6205, loss: 3.4915, Tree loss: 1.5041\nEpoch [65/100] - Loss: 0.9273, Accuracy: 0.8674\nAccuracy of the network on validation images: 0.6141, loss: 3.6081, Tree loss: 1.5208\nEpoch [66/100] - Loss: 0.9392, Accuracy: 0.8661\nAccuracy of the network on validation images: 0.6372, loss: 3.3513, Tree loss: 1.4422\nEpoch [67/100] - Loss: 0.9273, Accuracy: 0.8679\nAccuracy of the network on validation images: 0.6373, loss: 3.3425, Tree loss: 1.4416\nEpoch [68/100] - Loss: 0.9072, Accuracy: 0.8707\nAccuracy of the network on validation images: 0.6303, loss: 3.4069, Tree loss: 1.4638\nEpoch [69/100] - Loss: 0.9303, Accuracy: 0.8679\nAccuracy of the network on validation images: 0.6265, loss: 3.4461, Tree loss: 1.4713\nEpoch [70/100] - Loss: 0.8563, Accuracy: 0.8791\nAccuracy of the network on validation images: 0.6132, loss: 3.6679, Tree loss: 1.5603\nEpoch [71/100] - Loss: 0.9139, Accuracy: 0.8715\nAccuracy of the network on validation images: 0.6242, loss: 3.5581, Tree loss: 1.5111\nEpoch [72/100] - Loss: 0.9290, Accuracy: 0.8693\nAccuracy of the network on validation images: 0.6340, loss: 3.4228, Tree loss: 1.4636\nEpoch [73/100] - Loss: 0.9274, Accuracy: 0.8717\nAccuracy of the network on validation images: 0.6200, loss: 3.4800, Tree loss: 1.4915\nEpoch [74/100] - Loss: 0.9234, Accuracy: 0.8727\nAccuracy of the network on validation images: 0.6290, loss: 3.5069, Tree loss: 1.4758\nEpoch [75/100] - Loss: 0.8595, Accuracy: 0.8794\nAccuracy of the network on validation images: 0.6288, loss: 3.5682, Tree loss: 1.4984\nEpoch [76/100] - Loss: 0.8243, Accuracy: 0.8820\nAccuracy of the network on validation images: 0.6089, loss: 3.7297, Tree loss: 1.5829\nEpoch [77/100] - Loss: 0.8359, Accuracy: 0.8824\nAccuracy of the network on validation images: 0.6104, loss: 3.6717, Tree loss: 1.5417\nEpoch [78/100] - Loss: 0.8259, Accuracy: 0.8842\nAccuracy of the network on validation images: 0.6325, loss: 3.4912, Tree loss: 1.4533\nEpoch [79/100] - Loss: 0.7797, Accuracy: 0.8879\nAccuracy of the network on validation images: 0.6304, loss: 3.5910, Tree loss: 1.4946\nEpoch [80/100] - Loss: 0.8097, Accuracy: 0.8863\nAccuracy of the network on validation images: 0.6209, loss: 3.6556, Tree loss: 1.5284\nEpoch [81/100] - Loss: 0.8558, Accuracy: 0.8800\nAccuracy of the network on validation images: 0.6001, loss: 3.8244, Tree loss: 1.6033\nEpoch [82/100] - Loss: 0.8688, Accuracy: 0.8786\nAccuracy of the network on validation images: 0.5894, loss: 3.8839, Tree loss: 1.6361\nEpoch [83/100] - Loss: 0.8339, Accuracy: 0.8853\nAccuracy of the network on validation images: 0.5815, loss: 4.2477, Tree loss: 1.7115\nEpoch [84/100] - Loss: 0.8226, Accuracy: 0.8872\nAccuracy of the network on validation images: 0.6245, loss: 3.6091, Tree loss: 1.5143\nEpoch [85/100] - Loss: 0.7871, Accuracy: 0.8886\nAccuracy of the network on validation images: 0.6243, loss: 3.5066, Tree loss: 1.4908\nEpoch [86/100] - Loss: 0.7743, Accuracy: 0.8915\nAccuracy of the network on validation images: 0.6173, loss: 3.6684, Tree loss: 1.5353\nEpoch [87/100] - Loss: 0.7226, Accuracy: 0.8974\nAccuracy of the network on validation images: 0.6178, loss: 3.6367, Tree loss: 1.5150\nEpoch [88/100] - Loss: 0.7467, Accuracy: 0.8954\nAccuracy of the network on validation images: 0.6022, loss: 3.9278, Tree loss: 1.6108\nEpoch [89/100] - Loss: 0.7413, Accuracy: 0.8969\nAccuracy of the network on validation images: 0.6098, loss: 3.9178, Tree loss: 1.5539\nEpoch [90/100] - Loss: 0.7339, Accuracy: 0.8971\nAccuracy of the network on validation images: 0.6281, loss: 3.6068, Tree loss: 1.4819\nEpoch [91/100] - Loss: 0.7171, Accuracy: 0.8992\nAccuracy of the network on validation images: 0.6259, loss: 3.6446, Tree loss: 1.4998\nEpoch [92/100] - Loss: 0.7186, Accuracy: 0.9001\nAccuracy of the network on validation images: 0.6222, loss: 3.6846, Tree loss: 1.5124\nEpoch [93/100] - Loss: 0.7754, Accuracy: 0.8934\nAccuracy of the network on validation images: 0.6063, loss: 3.8758, Tree loss: 1.5900\nEpoch [94/100] - Loss: 0.7385, Accuracy: 0.8983\nAccuracy of the network on validation images: 0.6144, loss: 3.8094, Tree loss: 1.5857\nEpoch [95/100] - Loss: 0.7076, Accuracy: 0.9017\nAccuracy of the network on validation images: 0.6109, loss: 3.9282, Tree loss: 1.5646\nEpoch [96/100] - Loss: 0.7214, Accuracy: 0.9013\nAccuracy of the network on validation images: 0.6311, loss: 3.6885, Tree loss: 1.4989\nEpoch [97/100] - Loss: 0.6863, Accuracy: 0.9052\nAccuracy of the network on validation images: 0.6271, loss: 3.8301, Tree loss: 1.5333\nEpoch [98/100] - Loss: 0.6933, Accuracy: 0.9029\nAccuracy of the network on validation images: 0.6210, loss: 3.7423, Tree loss: 1.5032\nEpoch [99/100] - Loss: 0.6741, Accuracy: 0.9066\nAccuracy of the network on validation images: 0.6183, loss: 3.7871, Tree loss: 1.5474\nEpoch [100/100] - Loss: 0.7301, Accuracy: 0.9019\nAccuracy of the network on validation images: 0.6201, loss: 3.9714, Tree loss: 1.5536\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing**","metadata":{"id":"8Q01Kk42XffV"}},{"cell_type":"code","source":"\n#Testing\nmodel.load_state_dict(BEST_MODEL)\n\ntotal_test_step=len(test_loader)\n\nwith torch.no_grad():\n    test_acc=0\n    test_loss=0\n    Tree_Loss_Value=0\n\n    for i, (images, target) in enumerate(test_loader, 1):\n        \n        y_trans = target[0]\n        y_true = target[1]\n        \n        images = images.to(device)\n        y_true = y_true.to(device)\n        y_trans = y_trans.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        \n        # Loss\n        test_loss += criterion(outputs,y_trans)\n        Tree_Loss_Value += tLoss(outputs,y_true)\n        test_acc += accuracy(outputs[:,0:100], y_true)\n\n    print(f'Accuracy of the network on test images: {(test_acc/total_test_step):.4f}, loss: {(test_loss/total_test_step):.4f}, Tree loss: {(Tree_Loss_Value/total_test_step):.4f}')","metadata":{"id":"1eV5Njd2K3fn","execution":{"iopub.status.busy":"2022-09-22T05:21:08.081922Z","iopub.execute_input":"2022-09-22T05:21:08.082370Z","iopub.status.idle":"2022-09-22T05:21:27.083474Z","shell.execute_reply.started":"2022-09-22T05:21:08.082323Z","shell.execute_reply":"2022-09-22T05:21:27.082293Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Accuracy of the network on test images: 0.6301, loss: 3.8147, Tree loss: 1.4901\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"W50Zq-zsufa7"},"execution_count":null,"outputs":[]}]}